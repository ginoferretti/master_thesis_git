{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source notebook is loaded\n",
    "\n",
    "All the libraries and formulas needed for running the T-GARCH model are loaded. Please install all the libraries before running the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything from the converted script\n",
    "%run Source/T_GARCH.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-GARH model (Dropout=0)\n",
    "\n",
    "Model run for obtaining the training error of the T-GARCH model with dropout equal to 0. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2014 [00:00<?, ?it/s]c:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\arch\\__future__\\_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\arch\\__future__\\_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\arch\\__future__\\_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\arch\\__future__\\_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\arch\\__future__\\_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\arch\\__future__\\_utility.py:11: FutureWarning: \n",
      "The default for reindex is True. After September 2021 this will change to\n",
      "False. Set reindex to True or False to silence this message. Alternatively,\n",
      "you can use the import comment\n",
      "\n",
      "from arch.__future__ import reindexing\n",
      "\n",
      "to globally set reindex to True and silence this warning.\n",
      "\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2014 [02:24<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Close'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Close'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m Data \u001b[39m=\u001b[39m DatabaseGeneration (Database, Lag, LagSD)\n\u001b[0;32m     12\u001b[0m \u001b[39m#Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m T_ANN_ARCH_Model \u001b[39m=\u001b[39m T_ANN_ARCH_Fit(Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n\u001b[0;32m     14\u001b[0m \u001b[39m#VaR of ARCH models is computed\u001b[39;00m\n\u001b[0;32m     15\u001b[0m VaR_ARCH_Models\u001b[39m=\u001b[39mVaR_AR_Total(Alpha, T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGJR_GARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mTARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mEGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mAVGARCH_fit\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mFIGARCH_fit\u001b[39m\u001b[39m'\u001b[39m],T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mGJR_GARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mTARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mEGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mAVGARCH\u001b[39m\u001b[39m'\u001b[39m], T_ANN_ARCH_Model[\u001b[39m'\u001b[39m\u001b[39mFIGARCH\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\master_thesis_code\\master_thesis_git\\MultiTransformer_master\\MultiTransformer\\Train\\Source\\T_GARCH_utils.py:295\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Fit\u001b[1;34m(Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF, BatchSize)\u001b[0m\n\u001b[0;32m    293\u001b[0m model \u001b[39m=\u001b[39m Transformer_Model(XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], XData_AR_Norm_T\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], HeadsAttention\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, Dropout\u001b[39m=\u001b[39mDropout, LearningRate\u001b[39m=\u001b[39mLearningRate)\n\u001b[0;32m    294\u001b[0m model\u001b[39m.\u001b[39mfit(XData_AR_Norm_T, YData_AR_Norm_T, epochs\u001b[39m=\u001b[39mEpochs, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, batch_size\u001b[39m=\u001b[39mBatchSize); tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mclear_session()\n\u001b[1;32m--> 295\u001b[0m Forecast, Date_Forecast, TrainPrediction, ReturnForecast \u001b[39m=\u001b[39m T_ANN_ARCH_Forecast(Data, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model)\n\u001b[0;32m    296\u001b[0m VaR \u001b[39m=\u001b[39m T_ANN_ARCH_VaR(Alpha, Data[\u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m], Forecast,DF)\n\u001b[0;32m    297\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mT_ANN_ARCH_model\u001b[39m\u001b[39m'\u001b[39m:model, \u001b[39m'\u001b[39m\u001b[39mForecast_T_ANN_ARCH\u001b[39m\u001b[39m'\u001b[39m:Forecast, \u001b[39m'\u001b[39m\u001b[39mDate_Forecast\u001b[39m\u001b[39m'\u001b[39m:Date_Forecast, \u001b[39m'\u001b[39m\u001b[39mTrainPrediction\u001b[39m\u001b[39m'\u001b[39m: TrainPrediction, \u001b[39m'\u001b[39m\u001b[39mScaler\u001b[39m\u001b[39m'\u001b[39m: Scaled_Norm, \u001b[39m'\u001b[39m\u001b[39mForecast_GARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_GARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_GJR_GARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_GJR_GARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_TARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_TARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_EGARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_EGARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_AVGARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_AVGARCH, \u001b[39m'\u001b[39m\u001b[39mForecast_FIGARCH\u001b[39m\u001b[39m'\u001b[39m:For_CV_FIGARCH, \u001b[39m'\u001b[39m\u001b[39mReturnForecast\u001b[39m\u001b[39m'\u001b[39m:ReturnForecast, \u001b[39m'\u001b[39m\u001b[39mGARCH_fit\u001b[39m\u001b[39m'\u001b[39m: GARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mGJR_GARCH_fit\u001b[39m\u001b[39m'\u001b[39m:GJR_GARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mTARCH_fit\u001b[39m\u001b[39m'\u001b[39m:TARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mEGARCH_fit\u001b[39m\u001b[39m'\u001b[39m:EGARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mAVGARCH_fit\u001b[39m\u001b[39m'\u001b[39m:AVGARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mFIGARCH_fit\u001b[39m\u001b[39m'\u001b[39m:FIGARCH_Parameters, \u001b[39m'\u001b[39m\u001b[39mGARCH\u001b[39m\u001b[39m'\u001b[39m: GARCH, \u001b[39m'\u001b[39m\u001b[39mGJR_GARCH\u001b[39m\u001b[39m'\u001b[39m:GJR_GARCH, \u001b[39m'\u001b[39m\u001b[39mTARCH\u001b[39m\u001b[39m'\u001b[39m:TARCH, \u001b[39m'\u001b[39m\u001b[39mEGARCH\u001b[39m\u001b[39m'\u001b[39m:EGARCH, \u001b[39m'\u001b[39m\u001b[39mAVGARCH\u001b[39m\u001b[39m'\u001b[39m:AVGARCH, \u001b[39m'\u001b[39m\u001b[39mFIGARCH\u001b[39m\u001b[39m'\u001b[39m:FIGARCH, \u001b[39m'\u001b[39m\u001b[39mYData_Train\u001b[39m\u001b[39m'\u001b[39m:YData_AR_Norm_T, \u001b[39m'\u001b[39m\u001b[39mVaR\u001b[39m\u001b[39m'\u001b[39m: VaR}\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\master_thesis_code\\master_thesis_git\\MultiTransformer_master\\MultiTransformer\\Train\\Source\\T_GARCH_utils.py:224\u001b[0m, in \u001b[0;36mT_ANN_ARCH_Forecast\u001b[1;34m(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH, Scaled_Norm, XData_AR, model)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mT_ANN_ARCH_Forecast\u001b[39m(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH,Scaled_Norm, XData_AR, model):\n\u001b[1;32m--> 224\u001b[0m     XDataForecast, ReturnForecast \u001b[39m=\u001b[39m DatabaseGenerationForecast_AR(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\n\u001b[0;32m    225\u001b[0m     XDataForecast \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([XData_AR,XDataForecast])\n\u001b[0;32m    226\u001b[0m     XDataForecastTotalScaled \u001b[39m=\u001b[39m Scaled_Norm\u001b[39m.\u001b[39mtransform(XDataForecast)\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\master_thesis_code\\master_thesis_git\\MultiTransformer_master\\MultiTransformer\\Train\\Source\\T_GARCH_utils.py:215\u001b[0m, in \u001b[0;36mDatabaseGenerationForecast_AR\u001b[1;34m(Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDatabaseGenerationForecast_AR\u001b[39m (Database, Lag, LagSD, For_CV_GARCH, For_CV_GJR_GARCH, For_CV_TARCH, For_CV_EGARCH, For_CV_AVGARCH, For_CV_FIGARCH):\n\u001b[1;32m--> 215\u001b[0m     Data_Forecast\u001b[39m=\u001b[39mDatabaseGenerationForecast(Database, Lag, LagSD)\u001b[39m.\u001b[39miloc[(\u001b[39m-\u001b[39mLagSD\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[0;32m    216\u001b[0m     Index_Forecast\u001b[39m=\u001b[39mDatabaseGenerationForecast(Database, Lag, LagSD)\u001b[39m.\u001b[39mindex[(\u001b[39m-\u001b[39mLagSD\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)]\n\u001b[0;32m    217\u001b[0m     XDataForecast\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mSD\u001b[39m\u001b[39m'\u001b[39m: Data_Forecast[\u001b[39m'\u001b[39m\u001b[39mSD\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m: Data_Forecast[\u001b[39m'\u001b[39m\u001b[39mDailyReturnsOld\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m    218\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mCV_GARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_GARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_GJR_GARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_GJR_GARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_TARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_TARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \n\u001b[0;32m    219\u001b[0m                \u001b[39m'\u001b[39m\u001b[39mCV_EGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_EGARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_AVGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_AVGARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCV_FIGARCH\u001b[39m\u001b[39m'\u001b[39m : For_CV_FIGARCH\u001b[39m/\u001b[39m\u001b[39m100\u001b[39m}\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\master_thesis_code\\master_thesis_git\\MultiTransformer_master\\MultiTransformer\\Train\\Source\\T_GARCH_utils.py:205\u001b[0m, in \u001b[0;36mDatabaseGenerationForecast\u001b[1;34m(Database, Lag, LagSD)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mDatabaseGenerationForecast\u001b[39m (Database, Lag, LagSD):\n\u001b[1;32m--> 205\u001b[0m     DailyReturns, Index \u001b[39m=\u001b[39m ReturnCalculation(Database,Lag)\n\u001b[0;32m    206\u001b[0m     DailyReturnsOld \u001b[39m=\u001b[39m  np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39mnan, \u001b[39m1\u001b[39m),DailyReturns[\u001b[39m0\u001b[39m:(DailyReturns\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)])\n\u001b[0;32m    207\u001b[0m     SD \u001b[39m=\u001b[39m SDCalculation (DailyReturns, LagSD)\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\master_thesis_code\\master_thesis_git\\MultiTransformer_master\\MultiTransformer\\Train\\Source\\T_GARCH_utils.py:24\u001b[0m, in \u001b[0;36mReturnCalculation\u001b[1;34m(Database, lag)\u001b[0m\n\u001b[0;32m     22\u001b[0m dimension\u001b[39m=\u001b[39mDatabase\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m];dif\u001b[39m=\u001b[39mlag;Out\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros([dimension\u001b[39m-\u001b[39mdif])\n\u001b[0;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(dimension\u001b[39m-\u001b[39mdif):\n\u001b[1;32m---> 24\u001b[0m     Out[i]\u001b[39m=\u001b[39m(np\u001b[39m.\u001b[39mlog(Database[\u001b[39m'\u001b[39;49m\u001b[39mClose\u001b[39;49m\u001b[39m'\u001b[39;49m][i\u001b[39m+\u001b[39mdif])\u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mlog(Database[\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m][i]))\n\u001b[0;32m     25\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mrepeat(np\u001b[39m.\u001b[39mnan, dif),Out), Database\u001b[39m.\u001b[39mindex\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Xmoca\\OneDrive\\Desktop\\virtual_envs_folder\\thesis_venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Close'"
     ]
    }
   ],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit(Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('1. Drop000/4_T_GARCH.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-GARH model (Dropout=0.05)\n",
    "\n",
    "Model run for obtaining the training error of the T-GARCH model with dropout equal to 0.05. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.05; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('2. Drop005/4_T_GARCH.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-GARH model (Dropout=0.10)\n",
    "\n",
    "Model run for obtaining the training error of the T-GARCH model with dropout equal to 0.10. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.10; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('3. Drop010/4_T_GARCH.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-GARH model (Dropout=0.15)\n",
    "\n",
    "Model run for obtaining the training error of the T-GARCH model with dropout equal to 0.15. The forecasts and VaR values of the different models are saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index of end dates, database for validation and dataframe to collect the results are created. Model variables are defined.\n",
    "Start='2008-01-01'; End='2015-12-31'; IndexEndDays=yf.download(\"^GSPC\",start=Start,  end=End, progress=False).index\n",
    "Lag=1; LagSD=5; Timestep=10; Dropout=0.15; LearningRate=0.01; Epochs=5000; Alpha=0.005; DF=4\n",
    "DataValidation = DatabaseGeneration(yf.download(\"^GSPC\",start='2000-01-01', end=date.today()+timedelta(days=1), progress=False), Lag, LagSD)\n",
    "ResultsCollection=pd.DataFrame({'Date_Forecast': [], 'Forecast_T_ANN_ARCH': [],'Forecast_GARCH':[],'Forecast_GJR_GARCH':[], 'Forecast_TARCH':[],'Forecast_EGARCH':[],'Forecast_AVGARCH':[],'Forecast_FIGARCH':[],'ReturnForecast':[],'TrueSD':[], 'VaR_T_ANN_ARCH':[], 'VaR_GARCH':[], 'VaR_GJR_GARCH':[], 'VaR_TARCH':[], 'VaR_EGARCH':[], 'VaR_AVGARCH':[], 'VaR_FIGARCH':[]})\n",
    "#Loop for generating the results\n",
    "for i in tqdm(range(IndexEndDays.shape[0])):\n",
    "    #Database is downloaded from yahoo finance and lag of returns defined\n",
    "    Database=yf.download(\"^GSPC\",start=IndexEndDays[i].date()-timedelta(days=650), end=IndexEndDays[i].date(), progress=False)\n",
    "    #Database for fitting the models is generated\n",
    "    Data = DatabaseGeneration (Database, Lag, LagSD)\n",
    "    #Fitting of Transformed ANN-ARCH model, ARCH models and forecasting of the next volatility value\n",
    "    T_ANN_ARCH_Model = T_ANN_ARCH_Fit (Data, Lag, LagSD, Timestep, Dropout, LearningRate, Epochs, Alpha, DF)\n",
    "    #VaR of ARCH models is computed\n",
    "    VaR_ARCH_Models=VaR_AR_Total(Alpha, T_ANN_ARCH_Model['GARCH_fit'], T_ANN_ARCH_Model['GJR_GARCH_fit'], T_ANN_ARCH_Model['TARCH_fit'], T_ANN_ARCH_Model['EGARCH_fit'], T_ANN_ARCH_Model['AVGARCH_fit'], T_ANN_ARCH_Model['FIGARCH_fit'],T_ANN_ARCH_Model['GARCH'], T_ANN_ARCH_Model['GJR_GARCH'], T_ANN_ARCH_Model['TARCH'], T_ANN_ARCH_Model['EGARCH'], T_ANN_ARCH_Model['AVGARCH'], T_ANN_ARCH_Model['FIGARCH'])\n",
    "    #Results are collected\n",
    "    IterResults={'Date_Forecast': T_ANN_ARCH_Model['Date_Forecast'].date(), 'Forecast_T_ANN_ARCH': T_ANN_ARCH_Model['Forecast_T_ANN_ARCH'],'Forecast_GARCH':T_ANN_ARCH_Model['Forecast_GARCH']/100,'Forecast_GJR_GARCH':T_ANN_ARCH_Model['Forecast_GJR_GARCH']/100, 'Forecast_TARCH':T_ANN_ARCH_Model['Forecast_TARCH']/100,'Forecast_EGARCH':T_ANN_ARCH_Model['Forecast_EGARCH']/100,'Forecast_AVGARCH':T_ANN_ARCH_Model['Forecast_AVGARCH']/100,'Forecast_FIGARCH':T_ANN_ARCH_Model['Forecast_FIGARCH']/100,'ReturnForecast':T_ANN_ARCH_Model['ReturnForecast'],'TrueSD':DataValidation[DataValidation.index==pd.to_datetime(T_ANN_ARCH_Model['Date_Forecast'].date())]['TrueSD'][0], 'VaR_T_ANN_ARCH': T_ANN_ARCH_Model['VaR'], 'VaR_GARCH':VaR_ARCH_Models['VaR_GARCH'][0][0], 'VaR_GJR_GARCH':VaR_ARCH_Models['VaR_GJR_GARCH'][0][0], 'VaR_TARCH':VaR_ARCH_Models['VaR_TARCH'][0][0], 'VaR_EGARCH':VaR_ARCH_Models['VaR_EGARCH'][0][0], 'VaR_AVGARCH':VaR_ARCH_Models['VaR_AVGARCH'][0][0], 'VaR_FIGARCH':VaR_ARCH_Models['VaR_FIGARCH'][0][0]}\n",
    "    ResultsCollection=ResultsCollection.append(IterResults, ignore_index=True)\n",
    "    #Results are saved\n",
    "    ResultsCollection.to_csv('4. Drop015/4_T_GARCH.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
