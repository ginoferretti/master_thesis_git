# Multi-Transformer: A new neural network-based architecture for forecasting S&P volatility
Transformer layers have already been successfully applied for NLP purposes. This repository adapts Transfomer layers in order to be used within hybrid volatility forecasting models. Following the intuition of bagging, this repository also introduces Multi-Transformer layers. The aim of this novel architecture is to improve the stability and accurateness of Transformer layers by averaging multiple attention mechanism.

Here I readapt the work done in the paper to reach my master thesis goal: comparing NNs based algos and implement them into transformers, using GARCH and its variations as framework

